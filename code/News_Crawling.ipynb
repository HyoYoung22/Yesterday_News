{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_tag(sid, page):\n",
    "    ### 뉴스 분야(sid)와 페이지(page)를 입력하면 그에 대한 링크들을 리스트로 추출하는 함수 ###\n",
    "    \n",
    "    ## 1.\n",
    "    url = f\"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={sid}\"\\\n",
    "    \"#&date=%2000:00:00&page={page}\"\n",
    "    html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "    a_tag = soup.find_all(\"a\")\n",
    "    \n",
    "    ## 2.\n",
    "    tag_lst = []\n",
    "    for a in a_tag:\n",
    "        if \"href\" in a.attrs:  # href가 있는것만 고르는 것\n",
    "            if (f\"sid={sid}\" in a[\"href\"]) and (\"article\" in a[\"href\"]):\n",
    "                tag_lst.append(a[\"href\"])\n",
    "                \n",
    "    return tag_lst\n",
    "\n",
    "# 경제는 101, 사회는 102, IT/과학은 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_tag(sid):\n",
    "    re_lst = []\n",
    "    for i in range(10): # 오늘 기준으로 페이지 확인 \n",
    "        lst = ex_tag(sid, i+1)\n",
    "        re_lst.extend(lst)\n",
    "\n",
    "    # 중복 제거\n",
    "    re_set = set(re_lst)\n",
    "    re_lst = list(re_set)\n",
    "    \n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hrefs = {}\n",
    "sids = [102]  # 분야 리스트 - 경제는 101, 사회는 102, IT/과학은 105\n",
    "\n",
    "# 각 분야별로 링크 수집해서 딕셔너리에 저장\n",
    "for sid in sids:\n",
    "    sid_data = re_tag(sid)\n",
    "    all_hrefs[sid] = sid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_crawl(all_hrefs, sid, index):\n",
    "    \"\"\"\n",
    "    sid와 링크 인덱스를 넣으면 기사제목, 날짜, 본문을 크롤링하여 딕셔너리를 출력하는 함수 \n",
    "    \n",
    "    Args: \n",
    "        all_hrefs(dict): 각 분야별로 100페이지까지 링크를 수집한 딕셔너리 (key: 분야(sid), value: 링크)\n",
    "        sid(int): 분야 [101: 경제, 102: 사회, 105: IT/과학]\n",
    "        index(int): 링크의 인덱스\n",
    "    \n",
    "    Returns:\n",
    "        dict: 기사제목, 날짜, 본문이 크롤링된 딕셔너리\n",
    "    \n",
    "    \"\"\"\n",
    "    art_dic = {}\n",
    "    \n",
    "    ## 1.\n",
    "    title_selector = \"#title_area > span\"\n",
    "    date_selector = \"#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans\"\\\n",
    "    \"> div.media_end_head_info_datestamp > div:nth-child(1) > span\"\n",
    "    main_selector = \"#dic_area\"\n",
    "    \n",
    "    url = all_hrefs[sid][index]\n",
    "    html = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 \"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "    \n",
    "    ## 2.\n",
    "    # 제목 수집\n",
    "    title = soup.select(title_selector)\n",
    "    title_lst = [t.text for t in title]\n",
    "    title_str = \"\".join(title_lst)\n",
    "    \n",
    "    # 날짜 수집\n",
    "    date = soup.select(date_selector)\n",
    "    date_lst = [d.text for d in date]\n",
    "    date_str = \"\".join(date_lst)\n",
    "    \n",
    "    # 본문 수집\n",
    "    main = soup.select(main_selector)\n",
    "    main_lst = []\n",
    "    for m in main:\n",
    "        m_text = m.text\n",
    "        m_text = m_text.strip()\n",
    "        main_lst.append(m_text)\n",
    "    main_str = \"\".join(main_lst)\n",
    "    \n",
    "    ## 3.\n",
    "    art_dic[\"title\"] = title_str\n",
    "    art_dic[\"date\"] = date_str\n",
    "    art_dic[\"main\"] = main_str\n",
    "    \n",
    "    return art_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 섹션의 데이터 수집 (제목, 날짜, 본문, section, url)\n",
    "section_lst = [102]\n",
    "section = section_lst[0]\n",
    "artdic_lst = []\n",
    "\n",
    "#for section in tqdm(section_lst):\n",
    "for i in range(len(all_hrefs[section])):\n",
    "    art_dic = art_crawl(all_hrefs, section, i)\n",
    "    art_dic[\"section\"] = section\n",
    "    art_dic[\"url\"] = all_hrefs[section][i]\n",
    "    artdic_lst.append(art_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "art_df_so = pd.DataFrame(artdic_lst)\n",
    "art_df_so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 크롤링에 맞게 크롤링 저장\n",
    "art_df_so.to_csv(\"society.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_tag(sid, page):\n",
    "    ### 뉴스 분야(sid)와 페이지(page)를 입력하면 그에 대한 링크들을 리스트로 추출하는 함수 ###\n",
    "    \n",
    "    ## 1.\n",
    "    url = f\"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={sid}\"\\\n",
    "    \"#&date=%2000:00:00&page={page}\"\n",
    "    html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "    a_tag = soup.find_all(\"a\")\n",
    "    \n",
    "    ## 2.\n",
    "    tag_lst = []\n",
    "    for a in a_tag:\n",
    "        if \"href\" in a.attrs:  # href가 있는것만 고르는 것\n",
    "            if (f\"sid={sid}\" in a[\"href\"]) and (\"article\" in a[\"href\"]):\n",
    "                tag_lst.append(a[\"href\"])\n",
    "                \n",
    "    return tag_lst\n",
    "\n",
    "# 경제는 101, 사회는 102, IT/과학은 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_tag(sid):\n",
    "    re_lst = []\n",
    "    for i in range(10): # 오늘 기준으로 페이지 확인 \n",
    "        lst = ex_tag(sid, i+1)\n",
    "        re_lst.extend(lst)\n",
    "\n",
    "    # 중복 제거\n",
    "    re_set = set(re_lst)\n",
    "    re_lst = list(re_set)\n",
    "    \n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hrefs = {}\n",
    "sids = [101]  # 분야 리스트 - 경제는 101, 사회는 102, IT/과학은 105\n",
    "\n",
    "# 각 분야별로 링크 수집해서 딕셔너리에 저장\n",
    "for sid in sids:\n",
    "    sid_data = re_tag(sid)\n",
    "    all_hrefs[sid] = sid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_crawl(all_hrefs, sid, index):\n",
    "    \"\"\"\n",
    "    sid와 링크 인덱스를 넣으면 기사제목, 날짜, 본문을 크롤링하여 딕셔너리를 출력하는 함수 \n",
    "    \n",
    "    Args: \n",
    "        all_hrefs(dict): 각 분야별로 100페이지까지 링크를 수집한 딕셔너리 (key: 분야(sid), value: 링크)\n",
    "        sid(int): 분야 [101: 경제, 102: 사회, 105: IT/과학]\n",
    "        index(int): 링크의 인덱스\n",
    "    \n",
    "    Returns:\n",
    "        dict: 기사제목, 날짜, 본문이 크롤링된 딕셔너리\n",
    "    \n",
    "    \"\"\"\n",
    "    art_dic = {}\n",
    "    \n",
    "    ## 1.\n",
    "    title_selector = \"#title_area > span\"\n",
    "    date_selector = \"#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans\"\\\n",
    "    \"> div.media_end_head_info_datestamp > div:nth-child(1) > span\"\n",
    "    main_selector = \"#dic_area\"\n",
    "    \n",
    "    url = all_hrefs[sid][index]\n",
    "    html = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 \"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "    \n",
    "    ## 2.\n",
    "    # 제목 수집\n",
    "    title = soup.select(title_selector)\n",
    "    title_lst = [t.text for t in title]\n",
    "    title_str = \"\".join(title_lst)\n",
    "    \n",
    "    # 날짜 수집\n",
    "    date = soup.select(date_selector)\n",
    "    date_lst = [d.text for d in date]\n",
    "    date_str = \"\".join(date_lst)\n",
    "    \n",
    "    # 본문 수집\n",
    "    main = soup.select(main_selector)\n",
    "    main_lst = []\n",
    "    for m in main:\n",
    "        m_text = m.text\n",
    "        m_text = m_text.strip()\n",
    "        main_lst.append(m_text)\n",
    "    main_str = \"\".join(main_lst)\n",
    "    \n",
    "    ## 3.\n",
    "    art_dic[\"title\"] = title_str\n",
    "    art_dic[\"date\"] = date_str\n",
    "    art_dic[\"main\"] = main_str\n",
    "    \n",
    "    return art_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 섹션의 데이터 수집 (제목, 날짜, 본문, section, url)\n",
    "section_lst = [101]\n",
    "section = section_lst[0]\n",
    "artdic_lst = []\n",
    "\n",
    "#for section in tqdm(section_lst):\n",
    "for i in range(len(all_hrefs[section])):\n",
    "    art_dic = art_crawl(all_hrefs, section, i)\n",
    "    art_dic[\"section\"] = section\n",
    "    art_dic[\"url\"] = all_hrefs[section][i]\n",
    "    artdic_lst.append(art_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "art_df_eco = pd.DataFrame(artdic_lst)\n",
    "art_df_eco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 크롤링에 맞게 크롤링 저장\n",
    "art_df_eco.to_csv(\"economy.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_tag(sid, page):\n",
    "    ### 뉴스 분야(sid)와 페이지(page)를 입력하면 그에 대한 링크들을 리스트로 추출하는 함수 ###\n",
    "    \n",
    "    ## 1.\n",
    "    url = f\"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={sid}\"\\\n",
    "    \"#&date=%2000:00:00&page={page}\"\n",
    "    html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "    a_tag = soup.find_all(\"a\")\n",
    "    \n",
    "    ## 2.\n",
    "    tag_lst = []\n",
    "    for a in a_tag:\n",
    "        if \"href\" in a.attrs:  # href가 있는것만 고르는 것\n",
    "            if (f\"sid={sid}\" in a[\"href\"]) and (\"article\" in a[\"href\"]):\n",
    "                tag_lst.append(a[\"href\"])\n",
    "                \n",
    "    return tag_lst\n",
    "\n",
    "# 경제는 101, 사회는 102, IT/과학은 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_tag(sid):\n",
    "    re_lst = []\n",
    "    for i in range(10): # 오늘 기준으로 페이지 확인 \n",
    "        lst = ex_tag(sid, i+1)\n",
    "        re_lst.extend(lst)\n",
    "\n",
    "    # 중복 제거\n",
    "    re_set = set(re_lst)\n",
    "    re_lst = list(re_set)\n",
    "    \n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hrefs = {}\n",
    "sids = [105]  # 분야 리스트 - 경제는 101, 사회는 102, IT/과학은 105\n",
    "\n",
    "# 각 분야별로 링크 수집해서 딕셔너리에 저장\n",
    "for sid in sids:\n",
    "    sid_data = re_tag(sid)\n",
    "    all_hrefs[sid] = sid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_crawl(all_hrefs, sid, index):\n",
    "    \"\"\"\n",
    "    sid와 링크 인덱스를 넣으면 기사제목, 날짜, 본문을 크롤링하여 딕셔너리를 출력하는 함수 \n",
    "    \n",
    "    Args: \n",
    "        all_hrefs(dict): 각 분야별로 100페이지까지 링크를 수집한 딕셔너리 (key: 분야(sid), value: 링크)\n",
    "        sid(int): 분야 [101: 경제, 102: 사회, 105: IT/과학]\n",
    "        index(int): 링크의 인덱스\n",
    "    \n",
    "    Returns:\n",
    "        dict: 기사제목, 날짜, 본문이 크롤링된 딕셔너리\n",
    "    \n",
    "    \"\"\"\n",
    "    art_dic = {}\n",
    "    \n",
    "    ## 1.\n",
    "    title_selector = \"#title_area > span\"\n",
    "    date_selector = \"#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans\"\\\n",
    "    \"> div.media_end_head_info_datestamp > div:nth-child(1) > span\"\n",
    "    main_selector = \"#dic_area\"\n",
    "    \n",
    "    url = all_hrefs[sid][index]\n",
    "    html = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 \"\\\n",
    "    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\\\n",
    "    \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html.text, \"lxml\")\n",
    "    \n",
    "    ## 2.\n",
    "    # 제목 수집\n",
    "    title = soup.select(title_selector)\n",
    "    title_lst = [t.text for t in title]\n",
    "    title_str = \"\".join(title_lst)\n",
    "    \n",
    "    # 날짜 수집\n",
    "    date = soup.select(date_selector)\n",
    "    date_lst = [d.text for d in date]\n",
    "    date_str = \"\".join(date_lst)\n",
    "    \n",
    "    # 본문 수집\n",
    "    main = soup.select(main_selector)\n",
    "    main_lst = []\n",
    "    for m in main:\n",
    "        m_text = m.text\n",
    "        m_text = m_text.strip()\n",
    "        main_lst.append(m_text)\n",
    "    main_str = \"\".join(main_lst)\n",
    "    \n",
    "    ## 3.\n",
    "    art_dic[\"title\"] = title_str\n",
    "    art_dic[\"date\"] = date_str\n",
    "    art_dic[\"main\"] = main_str\n",
    "    \n",
    "    return art_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 섹션의 데이터 수집 (제목, 날짜, 본문, section, url)\n",
    "section_lst = [105]\n",
    "section = section_lst[0]\n",
    "artdic_lst = []\n",
    "\n",
    "#for section in tqdm(section_lst):\n",
    "for i in range(len(all_hrefs[section])):\n",
    "    art_dic = art_crawl(all_hrefs, section, i)\n",
    "    art_dic[\"section\"] = section\n",
    "    art_dic[\"url\"] = all_hrefs[section][i]\n",
    "    artdic_lst.append(art_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "art_df_it = pd.DataFrame(artdic_lst)\n",
    "art_df_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 크롤링에 맞게 크롤링 저장\n",
    "art_df_it.to_csv(\"IT.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스포츠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스포츠 제목\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = 'https://sports.news.naver.com/ranking/index.nhn?date=20230602'\n",
    "\n",
    "#step3.requests 패키지의 함수를 이용해 url의 html 문서를 가져온다.\n",
    "response = requests.get(url)\n",
    "html_text=response.text\n",
    "\n",
    "#step4.bs4 패키지의 함수를 이용해서 html 문서를 파싱한다.\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "#step6.bs4 패키지의 select 함수와 선택자 개념을 이용해서 뉴스기사 제목을 모두 가져온다.\n",
    "titles = soup.select('a.title')\n",
    "\n",
    "n=0\n",
    "for i in titles:\n",
    "    title = i.get_text()\n",
    "    print(title.strip())\n",
    "    n= n + 1\n",
    "    if(n==10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스포츠 뉴스\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = 'https://sports.news.naver.com/ranking/index'\n",
    "\n",
    "#step3.requests 패키지의 함수를 이용해 url의 html 문서를 가져온다.\n",
    "response = requests.get(url)\n",
    "html_text=response.text\n",
    "\n",
    "#step4.bs4 패키지의 함수를 이용해서 html 문서를 파싱한다.\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "#step6.bs4 패키지의 select 함수와 선택자 개념을 이용해서 뉴스기사 제목을 모두 가져온다.\n",
    "titles = soup.select('a.title')\n",
    "\n",
    "n=0\n",
    "news ={}\n",
    "for i in titles:\n",
    "    url = 'https://sports.news.naver.com/'+i.attrs[\"href\"]\n",
    "    response = requests.get(url, headers={'User-agent': 'Mozila/5.0'})      # to avoid error use headers\n",
    "    html = response.text                                                    # for each url get html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")                               # for each html make soup\n",
    "    headline = soup.select_one(\"div.news_headline\")\n",
    "    title = headline.select_one(\"h4\").text.strip()\n",
    "    #print(headline.select_one(\"h4\").text)\n",
    "    content = soup.select_one(\"#newsEndContents\")                                  # get the body\n",
    "#     print(content.text)\n",
    "    news[title] = (content.text.strip())\n",
    "    time.sleep(0.3)\n",
    "    n= n + 1\n",
    "    if(n==10):\n",
    "        break\n",
    "        \n",
    "(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'title':news.keys(),'content':news.values()})\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('sports.csv' ,encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연예 제목\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = 'https://entertain.naver.com/ranking'\n",
    "\n",
    "#step3.requests 패키지의 함수를 이용해 url의 html 문서를 가져온다.\n",
    "response = requests.get(url)\n",
    "html_text=response.text\n",
    "\n",
    "#step4.bs4 패키지의 함수를 이용해서 html 문서를 파싱한다.\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "#step6.bs4 패키지의 select 함수와 선택자 개념을 이용해서 뉴스기사 제목을 모두 가져온다.\n",
    "titles = soup.select('#ranking_news > div > div.rank_lst')\n",
    "\n",
    "n=0\n",
    "for i in titles:\n",
    "    title = i.get_text()\n",
    "    print(title.strip())\n",
    "#     n= n + 1\n",
    "#     if(n==10):\n",
    "#         break\n",
    "# titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연예 뉴스\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = 'https://entertain.naver.com/ranking'\n",
    "\n",
    "#step3.requests 패키지의 함수를 이용해 url의 html 문서를 가져온다.\n",
    "response = requests.get(url)\n",
    "html_text=response.text\n",
    "\n",
    "#step4.bs4 패키지의 함수를 이용해서 html 문서를 파싱한다.\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "#step6.bs4 패키지의 select 함수와 선택자 개념을 이용해서 뉴스기사 제목을 모두 가져온다.\n",
    "titles = soup.select('#ranking_news > div > div.rank_lst')\n",
    "newsList = (titles[0].select('a'))\n",
    "#n=0\n",
    "news ={}\n",
    "for i in newsList:\n",
    "#     print(i)\n",
    "    url = 'https://entertain.naver.com/'+i.attrs[\"href\"]\n",
    "    response = requests.get(url, headers={'User-agent': 'Mozila/5.0'})      # to avoid error use headers\n",
    "    html = response.text                                                    # for each url get html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")                               # for each html make soup\n",
    "    headline = soup.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    title = headline.text.strip()\n",
    "    #print(headline.text)\n",
    "    content = soup.select_one(\"#articeBody\")                                  # get the body\n",
    "    #print(content.text)\n",
    "    news[title] = (content.text.strip())\n",
    "    time.sleep(0.3)\n",
    "    #n= n + 1\n",
    "    #if(n==10):\n",
    "    #    break\n",
    "        \n",
    "(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'title':news.keys(),'content':news.values()})\n",
    "(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('entertain.csv' ,encoding='utf-8-sig',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
